{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Humo(u)r\n",
    "\n",
    "\"You know that scene in Animal House, it is a fellow playing folk music on a guitar, and John Belushi picks up the guitar and destroys it...Well, the British comedian would want to play a folk singer. We want to play the failure.\" \n",
    "\n",
    "--Stephen Fry (<a href='http://lybio.net/tag/stephen-fry-on-american-vs-british-comedy-transcription/'>interview+transcript</a>)\n",
    "\n",
    "\n",
    "Kudos to this super helpful <a href='http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/'>post on creating LSTMs in Keras</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle as pkl\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game plan\n",
    "Ok so the game plan for me then is to\n",
    "\n",
    "1. Identify unique words in the text (names, places, various entities) that could be used to distinguish shows and replace them with a generic identifier (xxNamexx)\n",
    "2. Format the text into workable chunks. Chunks of size 200 words should be good\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because British shows are notoriously short, I'll be combining The Office UK with Peep Show to get some extra data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=the-office-us\n",
    "http://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=the-office-uk\n",
    "http://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=peep-show-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_folders=['office_us']\n",
    "uk_folders=['office_uk','peep_show']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try to make sure the classifier doesn't just capitalize on differences in English vs. American spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_strings(x):\n",
    "    replace_dic={\" '\":\" \", \\\n",
    "                \"okay\":\"ok\",\\\n",
    "                 \"Okay\":\"ok\",\\\n",
    "                \"\\n\":\" \",\\\n",
    "                \"apologise\":\"apologize\",\\\n",
    "                \"humour\":\"humor\",\\\n",
    "                 \"colour\":\"color\",\\\n",
    "                 \"flavour\":\"flavor\",\\\n",
    "                 \"favour\":\"favor\",\\\n",
    "                 \"labour\":\"labor\",\\\n",
    "                 \"neighbour\":\"neighbor\",\\\n",
    "                 \"favourite\":\"favorite\",\\\n",
    "                 \"centre\":\"center\",\\\n",
    "                 \"fibre\":\"fiber\",\\\n",
    "                 \"litre\":\"liter\",\\\n",
    "                 \"theatre\":\"theater\",\\\n",
    "                 \"organise\":\"organize\",\\\n",
    "                 \"recognise\":\"recognize\",\\\n",
    "                 \"realise\":\"realize\",\\\n",
    "                 \"civilisation\":\"civilization\",\\\n",
    "                 \"analyse\":\"analyze\",\\\n",
    "                 \"defence\":\"defense\",\\\n",
    "                 \"offence\":\"offense\",\\\n",
    "                 \"licence\":\"license\",\\\n",
    "                 \"mum\":\"mom\",\\\n",
    "                 \"arsehole\":\"asshole\",\\\n",
    "                 \"tenner\":'ten',\\\n",
    "                 \"lads\":\"guys\",\\\n",
    "                 \"quid\":\"buck\",\\\n",
    "                 \"telly\":\"TV\",\\\n",
    "                 \" loo \": \"bathroom \",\\\n",
    "                 \" loo.\":\" bathroom.\",\\\n",
    "                 \"wanker \":\"bastard \",\\\n",
    "                 \"wanking \":\"screwing \",\\\n",
    "                 \"tits\":\"boobs\",\\\n",
    "                 \"wank \":\"bastard \",\\\n",
    "                 \"pint\":\"beer\",\\\n",
    "                 \"prick\":\"dick\",\\\n",
    "                 \"snog\":\"kiss\",\\\n",
    "                 \"crisps\":\"fries\",\\\n",
    "                 \"lazer\":\"laser\",\\\n",
    "                 \"twat\":\"jerk\",\n",
    "                 \"shag\":\"screw\"\\\n",
    "                 \n",
    "                }\n",
    "    for i in replace_dic.keys():\n",
    "        x=x.replace(i,replace_dic[i])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long should the passages be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parse_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uk_data=[]\n",
    "uk_show=[]\n",
    "for fold_name in uk_folders:\n",
    "    uk_fold=os.path.join('data',fold_name)\n",
    "    curr_files=os.listdir(uk_fold)\n",
    "    curr_files=[i for i in curr_files if i[-3:]=='txt']\n",
    "    for fname in curr_files:\n",
    "        curr_fname=os.path.join(uk_fold,fname)\n",
    "        with open(curr_fname,'rb') as curr_f:\n",
    "            temp=replace_strings(curr_f.read().lower())\n",
    "            uk_curr_text=unicode(temp.decode('utf-8'))\n",
    "            uk_show.append(fold_name)\n",
    "            uk_data.append(uk_curr_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i)/(parse_size*5) for i in uk_data]) # 5 is ~ average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_data=[]\n",
    "us_show=[]\n",
    "for fold_name in us_folders:\n",
    "    us_fold=os.path.join('data',fold_name)\n",
    "    curr_files=os.listdir(us_fold)\n",
    "    curr_files=[i for i in curr_files if i[-3:]=='txt']    \n",
    "    for fname in curr_files:\n",
    "        curr_fname=os.path.join(us_fold,fname)\n",
    "        with open(curr_fname,'rb') as curr_f:\n",
    "            temp=replace_strings(curr_f.read().lower())\n",
    "            us_curr_text=unicode(temp.decode('utf-8'))\n",
    "            us_show.append(fold_name)\n",
    "            us_data.append(us_curr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1516"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i)/(parse_size*5) for i in us_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Words for TF-IDF=1\n",
      "[[u'dwight']]\n",
      "New Words for TF-IDF=21\n",
      "[u'angela', u'cos', u'darryl', u'dobby', u'dunder', u'erin', u'gabe', u'gareth', u'jez', u'kelly', u'kevin', u'mifflin', u'phyllis', u'schrute', u'scranton', u'soph', u'sophie', u'stanley', u'suze', u'toby']\n",
      "New Words for TF-IDF=41\n",
      "[u'angus', u'arse', u'bollocks', u'cece', u'cock', u'corrigan', u'elena', u'fucked', u'gail', u'gerard', u'halpert', u'holly', u'ian', u'jo', u'mose', u'nellie', u'philly', u'sabre', u'scott', u'senator']\n",
      "New Words for TF-IDF=61\n",
      "[u'bloke', u'bullshit', u'california', u'ceo', u'deangelo', u'dobs', u'employee', u'erm', u'jlb', u'learned', u'lice', u'loads', u'mall', u'mural', u'packer', u'sarah', u'todd', u'toni', u'val', u'zahra']\n",
      "New Words for TF-IDF=81\n",
      "[u'blimey', u'dickhead', u'dundies', u'halloween', u'hay', u'lager', u'lf', u'luke', u'named', u'paedo', u'pennsylvania', u'phillip', u'printers', u'sofa', u'sorted', u'strangler', u'swindon', u'tallahassee', u'truck', u'wuphf']\n",
      "New Words for TF-IDF=101\n",
      "[u'accounting', u'angelo', u'barbara', u'belsnickel', u'bout', u'cottage', u'cupcakes', u'foz', u'frankfurt', u'gotten', u'kenneth', u'lame', u'nard', u'nim', u'oggy', u'pens', u'reckon', u'sally', u'sectioned', u'stag']\n",
      "New Words for TF-IDF=121\n",
      "[u'aberdeen', u'administrator', u'annex', u'broccoli', u'bunker', u'cally', u'cathy', u'cornell', u'finch', u'helene', u'hogg', u'parkour', u'potatoes', u'quiz', u'raises', u'ravi', u'shitting', u'treasure', u'wanker', u'wernham']\n",
      "New Words for TF-IDF=141\n",
      "[u'aj', u'barney', u'beet', u'corfu', u'daryl', u'gifts', u'glee', u'jaws', u'keith', u'lipton', u'natalie', u'paperwork', u'password', u'philadelphia', u'plant', u'plop', u'salesmen', u'sweeney', u'tacos', u'threesome']\n",
      "New Words for TF-IDF=161\n",
      "[u'anytime', u'award', u'cappella', u'cupboard', u'dicks', u'dooby', u'elevator', u'finchy', u'gerrard', u'gog', u'keenan', u'mexico', u'pasta', u'recyclops', u'redundancies', u'redundancy', u'saz', u'soldier', u'thay', u'urgh']\n",
      "New Words for TF-IDF=181\n",
      "[u'bam', u'bleep', u'bog', u'bongo', u'doof', u'farms', u'fucker', u'gettysburg', u'gum', u'gunny', u'hudson', u'isabel', u'jr', u'limo', u'mikanos', u'na', u'owner', u'shabooya', u'trash', u'walter']\n",
      "New Words for TF-IDF=201\n",
      "[u'39', u'ciao', u'com', u'grain', u'hippy', u'honda', u'jones', u'jordan', u'kapoor', u'lan', u'nance', u'napoleon', u'nate', u'oui', u'resume', u'rumor', u'saber', u'temperature', u'valerie', u'yo']\n",
      "New Words for TF-IDF=221\n",
      "[u'bennett', u'bloggers', u'brandon', u'cheat', u'counseling', u'cupcake', u'einsteins', u'exclaims', u'expert', u'gah', u'grayson', u'heather', u'iq', u'justine', u'lasagne', u'maternity', u'savage', u'shirley', u'yell', u'yoko']\n",
      "New Words for TF-IDF=241\n",
      "[u'beesly', u'computron', u'cynthia', u'flenderson', u'godl', u'hairy', u'hannon', u'ive', u'nicked', u'nursery', u'orgazoid', u'pamela', u'philbin', u'prices', u'snap', u'uk', u'vacation', u'vp', u'washington', u'zeus']\n",
      "New Words for TF-IDF=261\n",
      "[u'barber', u'behavior', u'burglar', u'cameras', u'cordray', u'courthouse', u'dartmouth', u'disabled', u'grotti', u'jada', u'juggling', u'limitless', u'limits', u'lobby', u'malcolm', u'mallard', u'nashua', u'palmer', u'recipe', u'rolf']\n",
      "New Words for TF-IDF=281\n",
      "[u'athlead', u'bands', u'blake', u'bong', u'buddies', u'camilla', u'coin', u'cooler', u'dildo', u'ew', u'grandfather', u'kay', u'kol', u'ls', u'oliver', u'orgones', u'pregnancy', u'soy', u'stalin', u'worries']\n",
      "New Words for TF-IDF=301\n",
      "[u'bagel', u'bastards', u'binghamton', u'blockbusters', u'booster', u'doin', u'fancied', u'fest', u'finance', u'foam', u'fuckin', u'kettering', u'kettle', u'lloyd', u'nelson', u'pledge', u'quantocks', u'sensei', u'steffan', u'swap']\n"
     ]
    }
   ],
   "source": [
    "tfidf_words=[]\n",
    "for j in range(1,302,20):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1,\n",
    "                                       max_features=j,\n",
    "                                       stop_words='english')\n",
    "#     tfidf_values=tfidf_vectorizer.fit_transform([' '.join(us_data), \\\n",
    "#                                                  ' '.join([i[0] for i in zip(uk_data,uk_show) if i[1]=='office_uk']), \\\n",
    "#                                                  ' '.join([i[0] for i in zip(uk_data,uk_show) if i[1]=='peep_show'])] \\\n",
    "#                                                )\n",
    "    tfidf_values=tfidf_vectorizer.fit_transform([' '.join(us_data), \\\n",
    "                                             ' '.join(uk_data)]\n",
    "                                           )\n",
    "    tfidf_words.append(tfidf_vectorizer.get_feature_names())\n",
    "    \n",
    "    print 'New Words for TF-IDF='+str(j)\n",
    "    if len(tfidf_words)>1:\n",
    "        print [i for i in tfidf_words[-1] if i not in tfidf_words[-2]]\n",
    "    else:\n",
    "        print tfidf_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we start off strong with names and then there's a mix of proper nouns. I'm going to remove the words in the first 100 and then the proper nouns in the 100-300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will need to revise this by hand if update data\n",
    "pronouns=['cornell','keenan','tallahassee','paula','philadelphia','shirley','todd', \\\n",
    "          'valerie','aberdeen','brandon','athlead','blake','camilla','nick','orgazoid', \\\n",
    "          'rolf','strangler','columbus','dave','obama','binghamton','drake','enya','beesly','iris', \\\n",
    "          'justine','nelson','bez','eva','bristol','lamar','nate','walter','anton','bertram','gerrard' \\\n",
    "          ,'oliver','norris','robert','michael','scott','pbs','barney','daryl','glee','keith','ravi', \\\n",
    "          'sarah','sweeney','lipton','natalie','nance','jordan','heather','malcolm', \\\n",
    "          'kapoor','hastings','jim','jeremy','andy','nashua','steffan','hannon', \\\n",
    "         'cally','helene','corfu','saz','mikanos','gettysburg','isabel','dartmouth','joshy']\n",
    "\n",
    "words_to_remove=tfidf_words[5]+pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proc_text(curr_data,words_to_remove):\n",
    "    parsed_data = nlp(curr_data)\n",
    "    lemma_data=[w.lemma_ for w in parsed_data if (w.pos_!='PUNCT' and w.pos_!='NUM' and w.pos_!='SPACE')]\n",
    "    rem_data=['xxNamexx' if w in words_to_remove else w for w in lemma_data]\n",
    "    return rem_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_data_proc=[' '.join(proc_text(doc,words_to_remove)) for doc in us_data]\n",
    "uk_data_proc=[' '.join(proc_text(doc,words_to_remove)) for doc in uk_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now uk/us_data_proc contains the text with the show-specific words replaced by xxNamexx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric vectorized representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now turn these texts into numbers that the algorithm can actually use. I'm going to use the CountVectorize function to find the 5000 most frequent words to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features=5000\n",
    "tf_vectorizer = CountVectorizer(min_df=2,max_features=n_features,stop_words='english')\n",
    "tf_vectorizer.fit(us_data_proc+uk_data_proc)\n",
    "tf_words=tf_vectorizer.get_feature_names()\n",
    "freqs=np.reshape(np.asarray(np.mean(tf_vectorizer.transform(us_data_proc+uk_data_proc),axis=0)),(n_features,))\n",
    "tf_words=[j[0] for j in sorted([i for i in zip(tf_words,freqs)],key=lambda x:x[1])[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_vec_us_fname='num_vec_data_us.pkl'\n",
    "if not os.path.exists(num_vec_us_fname):\n",
    "    us_data_vec=[[tf_words.index(w)+1 for w in proc_text(doc,words_to_remove) if w in tf_words] for doc in us_data]\n",
    "    with open(num_vec_us_fname,'wb') as f:\n",
    "        pkl.dump(us_data_vec,f)\n",
    "else:\n",
    "    with open(num_vec_us_fname,'rb') as f:\n",
    "        us_data_vec=pkl.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vec_uk_fname='num_vec_data_uk.pkl'\n",
    "if not os.path.exists(num_vec_uk_fname):\n",
    "    uk_data_vec=[[tf_words.index(w)+1 for w in proc_text(doc,words_to_remove) if w in tf_words] for doc in uk_data]\n",
    "    with open(num_vec_uk_fname,'wb') as f:\n",
    "        pkl.dump(uk_data_vec,f)\n",
    "else:\n",
    "    with open(num_vec_uk_fname,'rb') as f:\n",
    "        uk_data_vec=pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Split transcripts\n",
    "\n",
    "Divide transcripts into sections about **parse_size** words. Let UK=1, US=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_data=[]\n",
    "y_data=[]\n",
    "\n",
    "for doc in us_data_vec:\n",
    "    win_range=range(0,len(doc),parse_size)+[len(doc)]\n",
    "    for i in range(0,len(win_range)-1):\n",
    "        x_data.append(doc[win_range[i]:win_range[i+1]])\n",
    "        y_data.append(0)\n",
    "\n",
    "for doc in uk_data_vec:\n",
    "    win_range=range(0,len(doc),parse_size)+[len(doc)]\n",
    "    for i in range(0,len(win_range)-1):\n",
    "        x_data.append(doc[win_range[i]:win_range[i+1]])\n",
    "        y_data.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.461105904405\n",
      "1067\n"
     ]
    }
   ],
   "source": [
    "print np.mean(y_data)\n",
    "print len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length=max([len(i) for i in x_data])\n",
    "x_data2 = sequence.pad_sequences(x_data, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3048,  661, 2445, ...,   72,   11,  808],\n",
       "       [  16,   34,   34, ...,   66,   90,  191],\n",
       "       [4806,   63,   63, ...,    6, 1030,    6],\n",
       "       ..., \n",
       "       [  90, 1711,  169, ...,  226,  169, 3315],\n",
       "       [   3,   61,   92, ...,    3,   50,    3],\n",
       "       [   0,    0,    0, ...,   14, 1218,  734]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a simple Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "n_features=5000\n",
    "tf_vectorizer_nb = CountVectorizer(min_df=2,max_features=n_features,stop_words='english')\n",
    "nb_data=tf_vectorizer_nb.fit_transform(us_data_proc+uk_data_proc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_data_nb=np.asarray(nb_data.todense())\n",
    "y_data_nb=([0]*len(us_data_proc))+([1]*len(uk_data_proc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "b_nb=MultinomialNB()\n",
    "b_nb.fit(X_train,y_train)\n",
    "b_nb.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(x_data2,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 200, 32)       160032      embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 200, 32)       0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 100)           53200       dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 100)           0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             101         dropout_7[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 213333\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 800 samples, validate on 267 samples\n",
      "Epoch 1/6\n",
      "800/800 [==============================] - 15s - loss: 0.6676 - acc: 0.6225 - val_loss: 0.7589 - val_acc: 0.4719\n",
      "Epoch 2/6\n",
      "800/800 [==============================] - 14s - loss: 0.6731 - acc: 0.6537 - val_loss: 0.5394 - val_acc: 0.8839\n",
      "Epoch 3/6\n",
      "800/800 [==============================] - 13s - loss: 0.5404 - acc: 0.8575 - val_loss: 0.5881 - val_acc: 0.7940\n",
      "Epoch 4/6\n",
      "800/800 [==============================] - 14s - loss: 0.5016 - acc: 0.8237 - val_loss: 0.5508 - val_acc: 0.7640\n",
      "Epoch 5/6\n",
      "800/800 [==============================] - 14s - loss: 0.4841 - acc: 0.8125 - val_loss: 0.5501 - val_acc: 0.7715\n",
      "Epoch 6/6\n",
      "800/800 [==============================] - 13s - loss: 0.4952 - acc: 0.8000 - val_loss: 0.5327 - val_acc: 0.7790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18b082f50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model_single = Sequential()\n",
    "model_single.add(Embedding(n_features+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model_single.add(Dropout(0.2))\n",
    "model_single.add(LSTM(100))\n",
    "model_single.add(Dropout(0.2))\n",
    "model_single.add(Dense(1, activation='sigmoid'))\n",
    "model_single.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_single.summary())\n",
    "model_single.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Layer Model Accuracy\n",
      "Accuracy: 77.90%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_single.evaluate(X_test, y_test, verbose=0)\n",
    "print 'One Layer Model Accuracy'\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one layer model has pretty good accuracy! Let's try out two layers to see if there's information in longer-term depenencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layer stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 200, 32)       160032      embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 200, 32)       0           embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 200, 100)      53200       dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 200, 100)      0           lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 100)           80400       dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 100)           0           lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             101         dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 293733\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 800 samples, validate on 267 samples\n",
      "Epoch 1/8\n",
      "800/800 [==============================] - 31s - loss: 0.6978 - acc: 0.5150 - val_loss: 0.7033 - val_acc: 0.4270\n",
      "Epoch 2/8\n",
      "800/800 [==============================] - 28s - loss: 0.6390 - acc: 0.7200 - val_loss: 0.8432 - val_acc: 0.5506\n",
      "Epoch 3/8\n",
      "800/800 [==============================] - 28s - loss: 0.7320 - acc: 0.5075 - val_loss: 0.6910 - val_acc: 0.5393\n",
      "Epoch 4/8\n",
      "800/800 [==============================] - 29s - loss: 0.6254 - acc: 0.6837 - val_loss: 0.5504 - val_acc: 0.7116\n",
      "Epoch 5/8\n",
      "800/800 [==============================] - 28s - loss: 0.4713 - acc: 0.8125 - val_loss: 0.4451 - val_acc: 0.8090\n",
      "Epoch 6/8\n",
      "800/800 [==============================] - 29s - loss: 0.1781 - acc: 0.9500 - val_loss: 0.2181 - val_acc: 0.9288\n",
      "Epoch 7/8\n",
      "800/800 [==============================] - 28s - loss: 0.0921 - acc: 0.9725 - val_loss: 0.1801 - val_acc: 0.9476\n",
      "Epoch 8/8\n",
      "800/800 [==============================] - 28s - loss: 0.0737 - acc: 0.9850 - val_loss: 0.2020 - val_acc: 0.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18fc60890>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model_stack = Sequential()\n",
    "model_stack.add(Embedding(n_features+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model_stack.add(Dropout(0.2))\n",
    "model_stack.add(LSTM(100,return_sequences=True, input_shape=(max_review_length, n_features+1)))\n",
    "model_stack.add(Dropout(0.2))\n",
    "model_stack.add(LSTM(100))\n",
    "model_stack.add(Dropout(0.2))\n",
    "model_stack.add(Dense(1, activation='sigmoid'))\n",
    "model_stack.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_stack.summary())\n",
    "model_stack.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=8, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model Accuracy\n",
      "Accuracy: 94.38%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_stack.evaluate(X_test, y_test, verbose=0)\n",
    "print 'Stacked Model Accuracy'\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-layer LSTM performs VERY well. The improvement brought by adding the second LSTM layer also suggests that long-term information is very important for understanding which shows are British and which are American\n",
    "\n",
    "It may be worth testing out some other architectures (another layer, different number of hidden nodes), but given that this network is doing pretty damn good, let's test it out on some novel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classifying new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on two shows by the same creators: Parks and Rec and Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run on individual strings\n",
    "def clean_text(text,pn,tf_words):\n",
    "    # Takes in text, tokenizes, replaces proper nouns in pn with xxNamexx, lower cases, lemmatizes and encodes\n",
    "    # as numeric using tf_words\n",
    "    text=text.lower()\n",
    "    return [tf_words.index(w)+1 for w in proc_text(text,pn) if w in tf_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
